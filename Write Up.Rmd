---
title: "Project Write Up"
author: "Sophie Sommer"
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
```{r, echo=FALSE, include=FALSE, message=FALSE}
#load libraries
require(dplyr)
require(ggplot2)

#load data
finaldata <- read.csv(file="clean_data/Money Run 1FinalData.csv", row.names = 1)
questionsumm <- read.csv(file="clean_data/Money Run 1QuestionSumm.csv", row.names = 1)
learnersumm <- read.csv(file="clean_data/Money Run 1LearnerSumm.csv", row.names = 1)
respmat <- read.csv(file="clean_data/Money Run 1simpleatts.csv", row.names = 1)
```
  
# Rationale Behind the Project   
The purpose of this project is to better understand how students' response times on assessment questions are related to student and question level attributes. There are many reasons that teachers and/or developers of online learning systems might be interested in understanding, or even predicting student response times. For example, a teacher who is interested in designing a test to measure student ability might want to know how much time to allot for each question. To make this determination, it would be useful to investigate the following: 1) Do more "difficult" questions require more time on average (and how much more time)?, and 2) Do students with higher ability levels tend to respond to questions more quickly? 

With respect the second question above, there might be additional nuance: for example, previous research has shown that it may be possible to identify students who are cheating by looking for exceptionally low response times. For lower stakes assessments, or assessments where students are given additional opportunities to correct their work, it is also possible that quick responses are associated with guessing and/or giving up early. Thus, we might expect that stronger students tend to answer questions more quickly to a point, beyond which fast responses are actually related to negative learning behaviors, like cheating or giving up.

Finally, this research aims to understand whether student persistence (i.e., unwillingness to give up, even after getting a question wrong) might be additionally relevant in predicting response times. For example, it is possible that highly persistent students spend comparatively more time on test questions because they are also more exhaustive in understanding every aspect of the question and their answer. Conversely, it is possible that highly persistent students are also stronger students overall, and therefore tend to respond faster. In any case, understanding the relationship between response time and student persistence (holding student ability and question difficulty constant) is important when a) designing timed assessments and b) estimating student ability based on time in addition to correctness.  

# The Data
The data for this analysis comes from a massive open online course (MOOC) called Money in Business 1. My final analysis is based on data from 2387 students (I only included students who responded to at least one question, and for whom I could reasonably estimate attempt times). The full course includes 30 quiz questions, which were asked over the course of four weeks. The data that I used comes from three files:  
1. An enrollments file, which contains enrollment information for every student who signed up for the course  
2. A question and response file, which is structured such that every row corresponds to a single student responding to a single question (and contains columns indicating the time that a response was submitted and whether it was correct or not)  
3. A step activity file, which contains time stamps for every click to a new "step" or module within the course  

# Cleaning and Organizing the Data for Analysis
In the code --> Create_Files.R script, I have outlined the functions and process that I used to clean and organize the data. These steps are also outlined below:   
  
1. Run the AttemptNums function to create the file Money Run 1AttNums.csv. This function uses the original enrollments file and question response file to:  
    a) Create a data frame where every row represents a particular student's interaction with a particular question.   
    b) For every student and question, an NA means that the student did not attempt the question, a positive value represents the number of attempts until the student answered with the correct response, and a negative value represents the number of attempts that the student made before giving up.  
  
2. Run the SimpleAtts function to create the file Money Run 1simpleatts.csv. This function uses the data frame created in 1 to:  
    a) Create a data frame where rows are students, columns are questions, and values in each cell are 1, 0, or NA  
    b) 1 means that the question was answered correctly on the first try, 0 means incorrect on first try, and NA means that the question was not attempted.  
  
3. Run the function Atts1to4 to create the file Money Run 1AttTimes1to4.csv. This function uses the original enrollments, question response, and step activity files to recover attempt times for attempts 1-4. I chose to restrict all of my analysis to 4 attempts because very few people made more than that on any question. This function works as follows:   
    a) Uses the step activity file to recover the first time that students opened a step activity that involved questions.   
    b) For all questions that were not the first in a step activity, recovers the last submission time for the previous question (this is the very last submission time, even if the person made more than 4 attempts because they got the question right and then kept making more attempts).    
    c) Fills in all time stamps for attempts 1 through 4.  
    d) Uses time stamps for all attempts and last interaction with a previous question or step activity to populate 1st, 2nd, 3rd, and 4th attempt times.   
  
4. Run the GiveUpPercent function to create the file Money Run 1GiveUp.csv. This function uses the attempt numbers file created in 1 to evaluate the percent of opportunities that students used out of the total number of opportunities that they had to make another attempt on a problem (after an incorrect answer). So, if a student got a question correct on the third try, they had two opportunities to re-try the question after getting it wrong, and they used both. If a person made three attempts at a problem and never got it right, then they had three opportunities to retry the question after a wrong answer, but they only used two of them. In the resulting file:  
    a) Students who got all questions right on the first try, and therefore never had any opportunities to re-try a question that they got wrong, are coded as NAs in this file.  
  
5. Use the attempt numbers file from 1, the attempt times file from 3, and information about item difficulty (calculated using a Rasch model on the simple attempts file from 2) to create a question summary file (saved as Money Run 1QuestionSumm.csv) with the following information for each question:  
    a) PropAttemptedQ: proportion of learners who attempted the problem
    b) PropCorrect1stQ: proportion of learners who got the question correct on the first attempt (out of those who made at least one attempt)
    c) PropGaveUpQ: proportion of learners who never got the question correct (out of those who made at least one attempt)
    d) PropGaveUpAfterWrongQ: proportion of learners who never got the question correct (out of those who were wrong on the first attempt)
    e) MeanLogTimeAtt1Q: mean log time spent on students' first attempt for that question  
    f) RaschDiff: The difficulty of the question, as estimated from a Rasch model on the simple attempts file  
  
6. Use the the attempt times file from 3, simple attempt numbers from 2, and information about student ability estimated using the same Rasch model as above to create a learner summary file with the following information for each learner:  
    a) PropAttempted: proportion of questions that they attempted
    b) RaschAbility: their score based on a Rasch model
    c) PropCorrect1st: the proportion of questions that they answered correctly on the first try (out of the questions that they attempted)
    d) MeanLogTotalTime: mean log of the total time they spent across all questions that they attempted
    e) MeanLogTimeAtt1: mean log of the first attempt times across all questions that they attempted  
  
7. Use dplyr to join some of the above datasets to create a final dataset (saved as Money Run 1FinalData.csv), which has a separate row for every student's attempt on every question, with a column representing the student's first attempt time for that question, the student's overall Rasch score, the difficulty of the question, and the proportion of opportunities to retry a question that the student utilized.  

# Initial Data Exploration   

My initial data exploration involved the following discoveries:   
  
1. When looking at the distribution of attempt times (in seconds) vs. log attempt times, the log attempt times are much closer to normally distributed, whereas the regular attempt times are very right skewed. This influenced my decision to model log attempt time instead of regular attempt time. This can be seen in the following histograms, which show attempt times in seconds and log attempt times for the first question in this course:     
```{r, echo=FALSE}
questions <- questionsumm$Question
hist(finaldata$Att1_Time[finaldata$Question==questions[1]], 
     breaks = 100,xlab="time in seconds (first attempt)",
     main="Histogram of first attempt times on question 1 (in seconds)")
hist(log(finaldata$Att1_Time[finaldata$Question==questions[1]]), breaks = 100,xlab="time in seconds (first attempt)", main="Histogram of log first attempt times on question 1")
```  

2. It is fairly clear by looking at the question summary that easier questions take students less time (on average) than harder questions. In the following plot, mean log first attempt time on any given question is plotted against question difficulty based on Rasch model estimates:      
```{r, echo = FALSE}
plot(questionsumm$RaschDiff, 
     questionsumm$MeanLogTimeAtt1,
     xlab="Estimated Question Difficulty",
     ylab="Mean Log Time on 1st Attempt", 
     main="Mean time on first attempts vs. question difficulty",
     pch=16)
```
  
   
3. The relationship between student ability and mean log time on first attempts is slightly more complicated. The following plot shows mean log time on first attempts against estimated student ability (from a Rasch model). It looks like, for lower ability levels, mean log time on first attempts increases as students' estimated ability increases. However, for students whose estimated ability is greater than about -.5, mean log time on first attempt stabilizes or even decreases. This pattern is shown in the smoothed plot below:  
```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot(jitter(learnersumm$RaschAbility, factor=100), 
     learnersumm$MeanLogTimeAtt1,
     xlab="Rasch Model Score",
     ylab="Mean Log Time on 1st Attempt", 
     pch=16, cex=.5)

ggplot(data.frame(Score=learnersumm$RaschAbility, LogTime = learnersumm$MeanLogTimeAtt1)) + geom_smooth(aes(x = learnersumm$RaschAbility, y = learnersumm$MeanLogTimeAtt1), method="loess", se=TRUE) + labs(x="Rasch Model Score", y="Mean Log Time on First Attempt", title="Mean Log Time on First Attempts vs Estimated Ability")
```
   
# Final Modeling
Because I found that there appeared to be some quadratic relationship between time on first attempt and ability level, my initial model included a squared term for ability (as measured by the Rasch model). I also included an interaction term between ability and difficulty, because I hypothesized that the impact of question difficulty on response times would be different depending on student ability levels. Therefore, my first baseline model was as follows (note: I also ran the model on a dataset with missing data for PropUsedOpp removed so that the R squared value could be compared to the R squared in my next model):  
  
```{r}
finaldata$RaschAbilitySq <- finaldata$RaschAbility^2
fit_base <- lm(log(Att1_Time)~(RaschAbility+RaschAbilitySq)*RaschDiff, data=finaldata)
summary(fit_base)

#subsetting to only include non-missing data
finaldata2 <- finaldata[!is.na(finaldata$PropUsedOpp),]
#re-run same model without missing data:  
fit_base2 <- lm(log(Att1_Time)~(RaschAbility+RaschAbilitySq)*RaschDiff, data=finaldata2)
summary(fit_base2)
```
  
Note that all of the predictors are significant at a .05 significance level. As expected, higher difficulty questions tend to have slower response times. Also as expected, the coefficient on RaschAbility (which measures student ability) is positive, but the coefficient on RaschAbility^2 is negative, indicating that higher ability levels are associated with longer response times to a point, at which point higher abilities are associated with faster response times. All of the interaction terms are also significant, suggesting that the relationship between question difficulty and response time is different depending on student ability levels. Overall, these factors are accounting for just over 18% of the variation in log first attempts times. Therefore, these factors explain some of the variation in response time, but there is still a lot of extra noise.    

After running these baseline models, I then added my measure of student resiliency and re-ran the model.   
```{r}
#first alternate model
fit_M1 <- lm(log(Att1_Time)~(RaschAbility+RaschAbilitySq)*RaschDiff+PropUsedOpp, data=finaldata)
summary(fit_M1)
```
    
PropUsedOpp is also a significant (at a .05 level) positive predictor of response time, suggesting that students who take more advantage of additional attempts also tend to take longer on their first attempt. Adjusted R squared increased, but only by about 0.0006, so it's not totally clear whether the addition of PropUsedOpp accounts for enough additional variance in log response time to justify the more complex model.   
    
# Other explorations  
My initial interest when I first began this project was in understanding whether I could measure student resilience by looking at student response patterns. My hypothesis was that I could identify students who had given up by looking for quick, repeated responses. In this initial pursuit, I noted the following:  

The following table shows mean log response times for 1st through 4th attempts. As expected, students tend to spend less time on every subsequent attempt:   
```{r, echo=FALSE}
meantimes <- data.frame(MeanAtt1=round(log(mean(finaldata$Att1_Time,na.rm=T)),2),
                        MeanAtt2=round(log(mean(finaldata$Att2_Time,na.rm=T)),2),
                        MeanAtt3=round(log(mean(finaldata$Att3_Time,na.rm=T)),2),
                        MeanAtt4=round(log(mean(finaldata$Att4_Time,na.rm=T)),2))
colnames(meantimes) <- c("1st Att",
                         "2nd Att",
                         "3rd Att",
                         "4th Att")
knitr::kable(meantimes)
```
   
To take a closer look at students who were making many quick attempts, I subsetted the data to only include student-question pairs where the student made 3 or more attempts on the question and the mean time spent on non-first attempts was under 5 seconds. There were 544 question-student pairs that met this criterion (representing 237 unique learners answering all 30 of the unique questions). Among these students, I saw that mean log response time on first attempts was also much shorter than what I calculated for the entire dataset above. However, resiliency scores were similar among this group as compared to the whole population.       
```{r}
#subsetting the data: fastresp
finaldata$AvgTimeNotFirstAtt<-rowMeans(finaldata[,c(4:6)], na.rm=TRUE)
IDs <- finaldata$Atts>=3 & finaldata$AvgTimeNotFirstAtt<5
fastresp <- finaldata[IDs,]

length(unique(fastresp$Learner))#number of unique learners in this subset
length(unique(fastresp$Question))#number of unique questions in this subset

round(log(mean(fastresp$Att1_Time)),2)#mean log att 1 response time for this subset
round(mean(fastresp$PropUsedOpp,na.rm=T),2) #mean resiliency score (subsetted data)
round(mean(finaldata$PropUsedOpp,na.rm=T),2) #mean resiliency score (full data)
```
     
It is reasonable to assume that students who tend to answer quickly on second (or 3rd or fourth) attempts also tend to answer more quickly on first attempts. The plot below shows this positive correlation (with regression line in in red). However, it is also possible that this type of behavior is indicative of some other aspect of student persistence/carefulness (or lack thereof), which is also predictive of response times, beyond what is already captured in the resilience score used above.     
```{r, echo=FALSE}
atts12 <- finaldata[,c(3:4)]
atts12 <- na.omit(atts12)
plot(jitter(log(atts12$Att1_Time),factor=100), 
     jitter(log(atts12$Att2_Time),factor=100),
     xlab="Log Time Att 1",
     ylab="Log Time Att 2", 
     main="Log Attempt 2 Time vs. Log Attempt 1 Time",
     pch=16, cex=.4)
lmres <- lm(log(Att2_Time)~log(Att1_Time), data=atts12)
lmres_summ <- summary(lmres)
abline(lmres_summ$coefficients[1,1],lmres_summ$coefficients[2,1], col=2, lwd=3)
```
  
# Ideas for Further Analysis  
1. Based on the ideas presented in the previous section, it would be interesting to see if I could design a more sensitive measure of student resilience, which measures students' tendency to go into "guessing" mode after getting a question wrong, and see if I could use this measure to more accurately predict students' response times.  
2. There are multiple runs of the Money in Business course, so it would be interesting to run the same analysis with the subsequent versions of the course and see whether I observe the same patterns. Similarly, it would be interesting to try this with other courses and examine the degree to which the relationship between response time and question difficulty, student ability, and student persistence are different for those coures. It would also be interesting to run a similar analysis for questions that are higher-stakes or completed with time pressure.   

  



