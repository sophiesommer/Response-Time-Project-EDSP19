---
title: "Rasch Model Issues"
author: "Sophie Sommer"
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
    
   
```{r, message=FALSE}
#load libraries
require(edstan)
require(rstanarm)
require(mirt)
require(ltm)
require(dplyr)

#load data
questionsumm <- read.csv(file="../clean_data/Money Run 1QuestionSumm.csv", row.names = 1)
learnersumm <- read.csv(file="../clean_data/Money Run 1LearnerSumm.csv", row.names = 1)
respmat <- read.csv(file="../clean_data/Money Run 1simpleatts.csv", row.names = 1)
```
    
When I first tried to use a Rasch model (using the gpcm function in the ltm package) to estimate question difficulty and student ability, I got a relatively strange result: all of the estimated scores were negative (although they should be normally distributed and centered around 0, which is one of the assumptions of the model). All of the beta coefficients were also negative (which is why the scores were negative) 
```{r, cache=TRUE}
#fit a rasch model
rasch_fit <- ltm::gpcm(respmat[,1:30], constraint = "rasch") 

#estimate scores based off model fit (i.e., student ability)
scores <- ltm::factor.scores(rasch_fit, resp.patterns = respmat[,1:30])
scores <- scores$score.dat$z1

#view a histogram of estimated scores
hist(scores)

#also inspect estimated beta coefficients
rasch_fit$coefficients
```
  
On the advice of my professor, I also checked to see if mirt::mirt() would give anything different, but I got similar (and also negative) estimates for all of the beta coefficients.   
```{r, message=FALSE, cache=TRUE}
#get difficulty estimates from mirt package
mod <- mirt(respmat, 1, 'Rasch')
coef.mod <- coef(mod, IRTpars=TRUE, simplify=TRUE)

#inspect estimated beta coefficients
coef.mod$items[,2]
```
  
Next, also based on a suggestion from my professor, I tried fitting a 2-PL model (which assumes constant $\alpha$ accross all items, but does not restrict $\alpha=1$ for all items). This time, estimated ability values appeared roughly normally distributed (centered around zero), as I would have expected.   
```{r, cache=TRUE}
twopl_fit <- ltm::rasch(respmat[,1:30])
scores_twopl <- factor.scores(twopl_fit, resp.patterns = respmat[,1:30])
scores_twopl <- scores_twopl$score.dat$z1
hist(scores_twopl)
```
    
However, I now encountered a new problem. First, I compared students' estimated scores with their raw scores (proportion of questions answered correctly on the first try), and there was a positive correlation as I would have expected. Then, I compared the raw difficulty of the question (proportion of students who answered the question correctly on the first try) and compared to the estimated difficulty of each question. Now, the estimates no longer made sense. Questions with a higher proportion of students answering correctly on the first try should be "easier" and therefore have lower estimated difficulty. However, the reverse was true: there was a positive association between the proportion of students answering the question correctly and the estimated difficulty of the question. To date, I have no good explanation of why that would be and cannot seem to simulate a similar result.        
```{r, cache=TRUE}
respmat$score_2pl <- scores_twopl
respmat$scoreRaw <- rowMeans(respmat[,1:30],na.rm=T)
difficulties <- twopl_fit$coefficients[,1]
questionsumm$difficulty_2pl <- difficulties
plot(respmat$scoreRaw, respmat$score_2pl, 
     xlab= "Raw Score", ylab="Estimated Score(2pl)", 
     main="Comparing Raw Score to Estimated Score")
plot(questionsumm$PropCorrect1stQ, questionsumm$difficulty_2pl, 
     xlab= "Proportion Correct", ylab="Estimated Difficulty(2pl)", 
     main="Comparing Raw Difficulty to Estimated Difficulty")
```
   
Now, out of curiosity, I tried running a Bayesian model and taking the mean difficulty estimates and ability estimates from the resulting posterior distributions for each.  
```{r, warning=FALSE, message=FALSE, cache=TRUE}
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```
  
```{r, warning=FALSE, cache=TRUE}
#save data in the appropriate format
list <- irt_data(response_matrix = respmat[,1:30])
```

```{r, warning=FALSE, message=FALSE, cache=TRUE}
#fit the model
fit_rasch <- irt_stan(list, 
                      model = "rasch_latent_reg.stan",
                      refresh = 0,
                      iter = 300, 
                      chains = 4)
```
   
```{r, cache=TRUE}
#obtain mean beta and theta estimates
res <- as.data.frame(fit_rasch)
betas <- colMeans(res)[2419:2448]
thetas <- colMeans(res)[30:2416]
```
    
Now, when I plotted the mean estimated difficulties from the Bayesian Rasch model against the proportion of students answering the question correctly on the first try, I got a result that made sense: harder questions have a smaller proportion of students answering them correctly on the first try.  
```{r, cache=TRUE}
#check whether question difficulty estimates make sense given the data by comparing
#to proportion of students who attempted the question who answered correctly on their first try
estdiff <- data.frame(Question=as.numeric(substring(colnames(respmat[,1:30]),2)),
                      RaschDiff = betas)
questionsumm <- left_join(questionsumm,estdiff, by="Question")
```

```{r, cache=TRUE}
#this plot (with Bayesian estimates) looks as expected
plot(questionsumm$RaschDiff,questionsumm$PropCorrect, 
     xlab="Estimated Difficulty",
     ylab="Prop. of Stud. Who Answered Quest. Correctly",
     main="Fit using edstan")
```
     
Just to check, I also plotted estimated abilities (mean values from the posterior distributions of thetas from the Bayesian Rasch model) against raw scores, and these were still positively associated as expected. Therefore, I ended up using the estimates from the Bayesian approach in my final analysis. However, if anyone has other ideas for what's going on here, please let me know!  
```{r,fig.align="center", out.width = "225px",fig.keep='all', cache=TRUE}
#check whether ability (theta) estimates make sense given the data by comparing to 
#proportion of questions students answered correctly on their first try (out of those attempted)
learners <- rownames(respmat)
estability <- data.frame(Learner=learners,
                         RaschAbility = thetas)

learnersumm <- left_join(learnersumm,estability, by="Learner")

#note: this plot looks as expected
plot(learnersumm$RaschAbility,learnersumm$PropCorrect,
     xlab="Estimated Ability",
     ylab="Raw Score")
```
     